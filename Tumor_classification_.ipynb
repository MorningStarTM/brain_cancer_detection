{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MorningStarTM/brain_cancer_detection/blob/main/Tumor_classification_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dthQn83Z2kPE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import itertools\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from sklearn.utils import shuffle\n",
        "from glob import glob\n",
        "from tensorflow.keras.layers import*\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import*\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "T4teVOGM3Am1"
      },
      "outputs": [],
      "source": [
        "#parameters\n",
        "HEIGHT, WIDTH = 224, 224\n",
        "CHANNEL = 3\n",
        "num_class = 3\n",
        "batch_size = 64\n",
        "class_names = [\"glioma\", \"meningioma\", \"notumor\", \"pituitary\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnnXioXJ3LOH"
      },
      "source": [
        "#Data Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ox2OTSKc3Jvb"
      },
      "outputs": [],
      "source": [
        "def load_data(path, split=0.1):\n",
        "    images = shuffle(glob(os.path.join(path, \"*\", \"*.jpg\")))\n",
        "    \n",
        "    #size of split \n",
        "    split_size = int(len(images) * split)\n",
        "\n",
        "    #split the data\n",
        "    train_data, valid_data = train_test_split(images, test_size=split_size, random_state=42)\n",
        "    train_data, test_data = train_test_split(train_data, test_size=split_size, random_state=42)\n",
        "\n",
        "    return train_data, valid_data, test_data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uZB1usmn3SKW"
      },
      "outputs": [],
      "source": [
        "#process the image\n",
        "def process_image(path):\n",
        "    #decode the path\n",
        "    path = path.decode()\n",
        "    #read image\n",
        "    image = cv2.imread(path, cv2.IMREAD_COLOR)\n",
        "    #resize the image\n",
        "    image = cv2.resize(image, [224, 224])\n",
        "    #scale the image\n",
        "    image = image / 255.0\n",
        "    #change the data type of image\n",
        "    image = image.astype(np.float32)\n",
        "\n",
        "    #labeling the image\n",
        "    class_name = path.split(\"/\")[-2]\n",
        "    class_idx = class_names.index(class_name)\n",
        "    class_idx = np.array(class_idx, dtype=np.int32)\n",
        "\n",
        "    return image, class_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "CyncemCa3Vld"
      },
      "outputs": [],
      "source": [
        "def parse(path):\n",
        "    image, labels = tf.numpy_function(process_image, [path], (tf.float32, tf.int32))\n",
        "    labels = tf.one_hot(labels, 4)\n",
        "    image.set_shape([224, 224, 3])\n",
        "    labels.set_shape(4)\n",
        "  \n",
        "    return image, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Cgt4wkoX3X_V"
      },
      "outputs": [],
      "source": [
        "#tensorflow dataset\n",
        "def tf_dataset(images, batch=8):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((images))\n",
        "    dataset = dataset.map(parse)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(2)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLg-_DiG3a88"
      },
      "source": [
        "#Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dErEI4Hw3ZYl"
      },
      "outputs": [],
      "source": [
        "#input layer\n",
        "inputs = Input(shape=(WIDTH, HEIGHT, CHANNEL))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "DKGGELmz3cPs"
      },
      "outputs": [],
      "source": [
        "#convolutional layer\n",
        "conv_x = Conv2D(16, (3,3), activation='relu', padding='same', strides=(1,1), kernel_initializer='he_normal')(inputs)\n",
        "conv_x = BatchNormalization()(conv_x)\n",
        "conv_x = MaxPooling2D((2,2), strides=(2,2))(conv_x)\n",
        "conv_x = Conv2D(16, (3,3), activation='relu', padding='same', strides=(1,1), kernel_initializer='he_normal')(conv_x)\n",
        "\n",
        "\n",
        "conv_x = Conv2D(32, (3,3), activation='relu', padding='same', strides=(1,1), kernel_initializer='he_normal')(conv_x)\n",
        "conv_x = BatchNormalization()(conv_x)\n",
        "conv_x = MaxPooling2D((2,2), strides=(1,1))(conv_x)\n",
        "conv_x = Conv2D(32, (3,3), activation='relu', padding='same', strides=(1,1), kernel_initializer='he_normal')(conv_x)\n",
        "\n",
        "conv_x = Conv2D(64, (3,3), activation='relu', padding='same', strides=(1,1), kernel_initializer='he_normal')(conv_x)\n",
        "conv_x = BatchNormalization()(conv_x)\n",
        "conv_x = MaxPooling2D((2,2), strides=(2,2))(conv_x)\n",
        "conv_x = Conv2D(64, (3,3), activation='relu', padding='same', strides=(1,1), kernel_initializer='he_normal')(conv_x)\n",
        "\n",
        "conv_x = Conv2D(128, (3,3), activation='relu', padding='same', strides=(1,1), kernel_initializer='he_normal')(conv_x)\n",
        "conv_x = BatchNormalization()(conv_x)\n",
        "conv_x = MaxPooling2D((2,2), strides=(2,2))(conv_x)\n",
        "conv_x = Conv2D(128, (3,3), activation='relu', padding='same', strides=(1,1), kernel_initializer='he_normal')(conv_x)\n",
        "\n",
        "conv_x = Conv2D(256, (3,3), activation='relu', padding='same', strides=(1,1), kernel_initializer='he_normal')(conv_x)\n",
        "conv_x = BatchNormalization()(conv_x)\n",
        "conv_x = MaxPooling2D((2,2), strides=(2,2))(conv_x)\n",
        "conv_x = Conv2D(256, (3,3), activation='relu', padding='same', strides=(1,1), kernel_initializer='he_normal')(conv_x)\n",
        "\n",
        "conv_x = Conv2D(512, (3,3), activation='relu', padding='same', strides=(1,1), kernel_initializer='he_normal')(conv_x)\n",
        "conv_x = BatchNormalization()(conv_x)\n",
        "conv_x = MaxPooling2D((2,2), strides=(2,2))(conv_x)\n",
        "conv_x = Conv2D(512, (3,3), activation='relu', padding='same', strides=(1,1), kernel_initializer='he_normal')(conv_x)\n",
        "\n",
        "#flatting\n",
        "flatten = Flatten()(conv_x)\n",
        "conv_x = Dense(16, activation='relu')(flatten)\n",
        "con_x = Dropout(0.3)(conv_x)\n",
        "#adding Dense layer with number of class \n",
        "outputs = Dense(4, activation='softmax')(conv_x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "tmz0l_yZ4CS9"
      },
      "outputs": [],
      "source": [
        "model = Model(inputs=inputs, outputs=outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10v50j244GQV",
        "outputId": "af8aaffc-e373-452b-ed5d-a85195ba962b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 224, 224, 16)      448       \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 224, 224, 16)     64        \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 112, 112, 16)     0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 112, 112, 16)      2320      \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 112, 112, 32)      4640      \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 112, 112, 32)     128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 111, 111, 32)     0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 111, 111, 32)      9248      \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 111, 111, 64)      18496     \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 111, 111, 64)     256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 55, 55, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 55, 55, 64)        36928     \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 55, 55, 128)       73856     \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 55, 55, 128)      512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 27, 27, 128)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 27, 27, 128)       147584    \n",
            "                                                                 \n",
            " conv2d_8 (Conv2D)           (None, 27, 27, 256)       295168    \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 27, 27, 256)      1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPooling  (None, 13, 13, 256)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_9 (Conv2D)           (None, 13, 13, 256)       590080    \n",
            "                                                                 \n",
            " conv2d_10 (Conv2D)          (None, 13, 13, 512)       1180160   \n",
            "                                                                 \n",
            " batch_normalization_5 (Batc  (None, 13, 13, 512)      2048      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_5 (MaxPooling  (None, 6, 6, 512)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_11 (Conv2D)          (None, 6, 6, 512)         2359808   \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 18432)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 16)                294928    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 4)                 68        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,017,764\n",
            "Trainable params: 5,015,748\n",
            "Non-trainable params: 2,016\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ZDtf0KTF4gQT"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-6), loss=\"categorical_crossentropy\", metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "vzMUtv7n5aVF"
      },
      "outputs": [],
      "source": [
        "model_file = \"/content/drive/MyDrive/CNN_Models/tumor_classfication_1.h5\"\n",
        "csv_file = \"/content/drive/MyDrive/Model CSV/tumor_classification.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Q_sCvQqz42Jl"
      },
      "outputs": [],
      "source": [
        "#initialize the callbakcs\n",
        "callbacks = [\n",
        "    ModelCheckpoint(model_file, verbose=1, save_best_only=True),\n",
        "    CSVLogger(csv_file),\n",
        "    ReduceLROnPlateau(monitor='val_accuracy', factor=0.1, patience=5, min_lr=1e-6, verbose=1)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "aFBRdZoz5YWN"
      },
      "outputs": [],
      "source": [
        "data_path = \"/content/drive/MyDrive/DataSet/brain_cancer/Training\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lv-qzDGk7vAt",
        "outputId": "42a91419-af40-4bbf-d03f-868faad2e4dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train:4586 - Test:573 - Valid:573\n"
          ]
        }
      ],
      "source": [
        "#split the dataset\n",
        "x_train, x_valid, x_test = load_data(data_path)\n",
        "print(f\"Train:{len(x_train)} - Test:{len(x_test)} - Valid:{len(x_valid)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "pB8mmzzP756G"
      },
      "outputs": [],
      "source": [
        "#tensor dataset\n",
        "train_df = tf_dataset(x_train)\n",
        "test_df = tf_dataset(x_test)\n",
        "valid_df = tf_dataset(x_valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-OobjVj7-pl",
        "outputId": "ba653f51-002c-4fba-8219-561196130823"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 1.5184 - accuracy: 0.3251\n",
            "Epoch 1: val_loss improved from inf to 1.39548, saving model to /content/drive/MyDrive/CNN_Models/tumor_classfication_1.h5\n",
            "72/72 [==============================] - 49s 515ms/step - loss: 1.5184 - accuracy: 0.3251 - val_loss: 1.3955 - val_accuracy: 0.2269 - lr: 1.0000e-06\n",
            "Epoch 2/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 1.2585 - accuracy: 0.4274\n",
            "Epoch 2: val_loss did not improve from 1.39548\n",
            "72/72 [==============================] - 33s 459ms/step - loss: 1.2585 - accuracy: 0.4274 - val_loss: 1.4002 - val_accuracy: 0.2426 - lr: 1.0000e-06\n",
            "Epoch 3/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 1.1344 - accuracy: 0.5020\n",
            "Epoch 3: val_loss did not improve from 1.39548\n",
            "72/72 [==============================] - 32s 446ms/step - loss: 1.1344 - accuracy: 0.5020 - val_loss: 1.3991 - val_accuracy: 0.2443 - lr: 1.0000e-06\n",
            "Epoch 4/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 1.0330 - accuracy: 0.5665\n",
            "Epoch 4: val_loss improved from 1.39548 to 1.34330, saving model to /content/drive/MyDrive/CNN_Models/tumor_classfication_1.h5\n",
            "72/72 [==============================] - 32s 449ms/step - loss: 1.0330 - accuracy: 0.5665 - val_loss: 1.3433 - val_accuracy: 0.2914 - lr: 1.0000e-06\n",
            "Epoch 5/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.9456 - accuracy: 0.6114\n",
            "Epoch 5: val_loss improved from 1.34330 to 1.25436, saving model to /content/drive/MyDrive/CNN_Models/tumor_classfication_1.h5\n",
            "72/72 [==============================] - 33s 449ms/step - loss: 0.9456 - accuracy: 0.6114 - val_loss: 1.2544 - val_accuracy: 0.4433 - lr: 1.0000e-06\n",
            "Epoch 6/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.8699 - accuracy: 0.6472\n",
            "Epoch 6: val_loss improved from 1.25436 to 1.13011, saving model to /content/drive/MyDrive/CNN_Models/tumor_classfication_1.h5\n",
            "72/72 [==============================] - 33s 454ms/step - loss: 0.8699 - accuracy: 0.6472 - val_loss: 1.1301 - val_accuracy: 0.5253 - lr: 1.0000e-06\n",
            "Epoch 7/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.8040 - accuracy: 0.6819\n",
            "Epoch 7: val_loss improved from 1.13011 to 1.01404, saving model to /content/drive/MyDrive/CNN_Models/tumor_classfication_1.h5\n",
            "72/72 [==============================] - 33s 453ms/step - loss: 0.8040 - accuracy: 0.6819 - val_loss: 1.0140 - val_accuracy: 0.5794 - lr: 1.0000e-06\n",
            "Epoch 8/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.7461 - accuracy: 0.7098\n",
            "Epoch 8: val_loss improved from 1.01404 to 0.92099, saving model to /content/drive/MyDrive/CNN_Models/tumor_classfication_1.h5\n",
            "72/72 [==============================] - 34s 465ms/step - loss: 0.7461 - accuracy: 0.7098 - val_loss: 0.9210 - val_accuracy: 0.6300 - lr: 1.0000e-06\n",
            "Epoch 9/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.6938 - accuracy: 0.7344\n",
            "Epoch 9: val_loss improved from 0.92099 to 0.85763, saving model to /content/drive/MyDrive/CNN_Models/tumor_classfication_1.h5\n",
            "72/72 [==============================] - 33s 451ms/step - loss: 0.6938 - accuracy: 0.7344 - val_loss: 0.8576 - val_accuracy: 0.6475 - lr: 1.0000e-06\n",
            "Epoch 10/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.6472 - accuracy: 0.7584\n",
            "Epoch 10: val_loss improved from 0.85763 to 0.77418, saving model to /content/drive/MyDrive/CNN_Models/tumor_classfication_1.h5\n",
            "72/72 [==============================] - 32s 441ms/step - loss: 0.6472 - accuracy: 0.7584 - val_loss: 0.7742 - val_accuracy: 0.6632 - lr: 1.0000e-06\n",
            "Epoch 11/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.6058 - accuracy: 0.7769\n",
            "Epoch 11: val_loss improved from 0.77418 to 0.71985, saving model to /content/drive/MyDrive/CNN_Models/tumor_classfication_1.h5\n",
            "72/72 [==============================] - 33s 456ms/step - loss: 0.6058 - accuracy: 0.7769 - val_loss: 0.7199 - val_accuracy: 0.6911 - lr: 1.0000e-06\n",
            "Epoch 12/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.5684 - accuracy: 0.7935\n",
            "Epoch 12: val_loss improved from 0.71985 to 0.69512, saving model to /content/drive/MyDrive/CNN_Models/tumor_classfication_1.h5\n",
            "72/72 [==============================] - 33s 454ms/step - loss: 0.5684 - accuracy: 0.7935 - val_loss: 0.6951 - val_accuracy: 0.7120 - lr: 1.0000e-06\n",
            "Epoch 13/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.5344 - accuracy: 0.8081\n",
            "Epoch 13: val_loss improved from 0.69512 to 0.67420, saving model to /content/drive/MyDrive/CNN_Models/tumor_classfication_1.h5\n",
            "72/72 [==============================] - 33s 452ms/step - loss: 0.5344 - accuracy: 0.8081 - val_loss: 0.6742 - val_accuracy: 0.7295 - lr: 1.0000e-06\n",
            "Epoch 14/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.5033 - accuracy: 0.8266\n",
            "Epoch 14: val_loss improved from 0.67420 to 0.65699, saving model to /content/drive/MyDrive/CNN_Models/tumor_classfication_1.h5\n",
            "72/72 [==============================] - 34s 463ms/step - loss: 0.5033 - accuracy: 0.8266 - val_loss: 0.6570 - val_accuracy: 0.7365 - lr: 1.0000e-06\n",
            "Epoch 15/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.4749 - accuracy: 0.8419\n",
            "Epoch 15: val_loss improved from 0.65699 to 0.64087, saving model to /content/drive/MyDrive/CNN_Models/tumor_classfication_1.h5\n",
            "72/72 [==============================] - 33s 458ms/step - loss: 0.4749 - accuracy: 0.8419 - val_loss: 0.6409 - val_accuracy: 0.7487 - lr: 1.0000e-06\n",
            "Epoch 16/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.4489 - accuracy: 0.8526\n",
            "Epoch 16: val_loss improved from 0.64087 to 0.62637, saving model to /content/drive/MyDrive/CNN_Models/tumor_classfication_1.h5\n",
            "72/72 [==============================] - 33s 452ms/step - loss: 0.4489 - accuracy: 0.8526 - val_loss: 0.6264 - val_accuracy: 0.7452 - lr: 1.0000e-06\n",
            "Epoch 17/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.4248 - accuracy: 0.8635\n",
            "Epoch 17: val_loss improved from 0.62637 to 0.61288, saving model to /content/drive/MyDrive/CNN_Models/tumor_classfication_1.h5\n",
            "72/72 [==============================] - 33s 453ms/step - loss: 0.4248 - accuracy: 0.8635 - val_loss: 0.6129 - val_accuracy: 0.7522 - lr: 1.0000e-06\n",
            "Epoch 18/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.4026 - accuracy: 0.8740\n",
            "Epoch 18: val_loss improved from 0.61288 to 0.60022, saving model to /content/drive/MyDrive/CNN_Models/tumor_classfication_1.h5\n",
            "72/72 [==============================] - 33s 459ms/step - loss: 0.4026 - accuracy: 0.8740 - val_loss: 0.6002 - val_accuracy: 0.7557 - lr: 1.0000e-06\n",
            "Epoch 19/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.3819 - accuracy: 0.8853\n",
            "Epoch 19: val_loss improved from 0.60022 to 0.58850, saving model to /content/drive/MyDrive/CNN_Models/tumor_classfication_1.h5\n",
            "72/72 [==============================] - 33s 461ms/step - loss: 0.3819 - accuracy: 0.8853 - val_loss: 0.5885 - val_accuracy: 0.7661 - lr: 1.0000e-06\n",
            "Epoch 20/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.3628 - accuracy: 0.8942\n",
            "Epoch 20: val_loss improved from 0.58850 to 0.57813, saving model to /content/drive/MyDrive/CNN_Models/tumor_classfication_1.h5\n",
            "72/72 [==============================] - 33s 451ms/step - loss: 0.3628 - accuracy: 0.8942 - val_loss: 0.5781 - val_accuracy: 0.7696 - lr: 1.0000e-06\n",
            "Epoch 21/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.3448 - accuracy: 0.9025\n",
            "Epoch 21: val_loss improved from 0.57813 to 0.56837, saving model to /content/drive/MyDrive/CNN_Models/tumor_classfication_1.h5\n",
            "72/72 [==============================] - 31s 434ms/step - loss: 0.3448 - accuracy: 0.9025 - val_loss: 0.5684 - val_accuracy: 0.7749 - lr: 1.0000e-06\n",
            "Epoch 22/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.3281 - accuracy: 0.9099\n",
            "Epoch 22: val_loss improved from 0.56837 to 0.55941, saving model to /content/drive/MyDrive/CNN_Models/tumor_classfication_1.h5\n",
            "72/72 [==============================] - 33s 452ms/step - loss: 0.3281 - accuracy: 0.9099 - val_loss: 0.5594 - val_accuracy: 0.7784 - lr: 1.0000e-06\n",
            "Epoch 23/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.3124 - accuracy: 0.9154\n",
            "Epoch 23: val_loss improved from 0.55941 to 0.55072, saving model to /content/drive/MyDrive/CNN_Models/tumor_classfication_1.h5\n",
            "72/72 [==============================] - 31s 434ms/step - loss: 0.3124 - accuracy: 0.9154 - val_loss: 0.5507 - val_accuracy: 0.7801 - lr: 1.0000e-06\n",
            "Epoch 24/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.2976 - accuracy: 0.9230\n",
            "Epoch 24: val_loss improved from 0.55072 to 0.54264, saving model to /content/drive/MyDrive/CNN_Models/tumor_classfication_1.h5\n",
            "72/72 [==============================] - 31s 435ms/step - loss: 0.2976 - accuracy: 0.9230 - val_loss: 0.5426 - val_accuracy: 0.7888 - lr: 1.0000e-06\n",
            "Epoch 25/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.2839 - accuracy: 0.9278\n",
            "Epoch 25: val_loss improved from 0.54264 to 0.53507, saving model to /content/drive/MyDrive/CNN_Models/tumor_classfication_1.h5\n",
            "72/72 [==============================] - 31s 434ms/step - loss: 0.2839 - accuracy: 0.9278 - val_loss: 0.5351 - val_accuracy: 0.7923 - lr: 1.0000e-06\n",
            "Epoch 26/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.2709 - accuracy: 0.9333\n",
            "Epoch 26: val_loss improved from 0.53507 to 0.52756, saving model to /content/drive/MyDrive/CNN_Models/tumor_classfication_1.h5\n",
            "72/72 [==============================] - 33s 459ms/step - loss: 0.2709 - accuracy: 0.9333 - val_loss: 0.5276 - val_accuracy: 0.8028 - lr: 1.0000e-06\n",
            "Epoch 27/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.2586 - accuracy: 0.9385\n",
            "Epoch 27: val_loss improved from 0.52756 to 0.52079, saving model to /content/drive/MyDrive/CNN_Models/tumor_classfication_1.h5\n",
            "72/72 [==============================] - 32s 443ms/step - loss: 0.2586 - accuracy: 0.9385 - val_loss: 0.5208 - val_accuracy: 0.8063 - lr: 1.0000e-06\n",
            "Epoch 28/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.2469 - accuracy: 0.9435\n",
            "Epoch 28: val_loss improved from 0.52079 to 0.51416, saving model to /content/drive/MyDrive/CNN_Models/tumor_classfication_1.h5\n",
            "72/72 [==============================] - 32s 438ms/step - loss: 0.2469 - accuracy: 0.9435 - val_loss: 0.5142 - val_accuracy: 0.8098 - lr: 1.0000e-06\n",
            "Epoch 29/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.2359 - accuracy: 0.9474\n",
            "Epoch 29: val_loss improved from 0.51416 to 0.50817, saving model to /content/drive/MyDrive/CNN_Models/tumor_classfication_1.h5\n",
            "72/72 [==============================] - 33s 459ms/step - loss: 0.2359 - accuracy: 0.9474 - val_loss: 0.5082 - val_accuracy: 0.8098 - lr: 1.0000e-06\n",
            "Epoch 30/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.2254 - accuracy: 0.9518\n",
            "Epoch 30: val_loss improved from 0.50817 to 0.50252, saving model to /content/drive/MyDrive/CNN_Models/tumor_classfication_1.h5\n",
            "72/72 [==============================] - 33s 455ms/step - loss: 0.2254 - accuracy: 0.9518 - val_loss: 0.5025 - val_accuracy: 0.8133 - lr: 1.0000e-06\n",
            "Epoch 31/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.2154 - accuracy: 0.9555\n",
            "Epoch 31: val_loss improved from 0.50252 to 0.49683, saving model to /content/drive/MyDrive/CNN_Models/tumor_classfication_1.h5\n",
            "72/72 [==============================] - 33s 456ms/step - loss: 0.2154 - accuracy: 0.9555 - val_loss: 0.4968 - val_accuracy: 0.8150 - lr: 1.0000e-06\n",
            "Epoch 32/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.2058 - accuracy: 0.9592\n",
            "Epoch 32: val_loss improved from 0.49683 to 0.49144, saving model to /content/drive/MyDrive/CNN_Models/tumor_classfication_1.h5\n",
            "72/72 [==============================] - 34s 469ms/step - loss: 0.2058 - accuracy: 0.9592 - val_loss: 0.4914 - val_accuracy: 0.8202 - lr: 1.0000e-06\n",
            "Epoch 33/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.1968 - accuracy: 0.9623\n",
            "Epoch 33: val_loss improved from 0.49144 to 0.48660, saving model to /content/drive/MyDrive/CNN_Models/tumor_classfication_1.h5\n",
            "72/72 [==============================] - 33s 450ms/step - loss: 0.1968 - accuracy: 0.9623 - val_loss: 0.4866 - val_accuracy: 0.8220 - lr: 1.0000e-06\n",
            "Epoch 34/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.1882 - accuracy: 0.9649\n",
            "Epoch 34: val_loss improved from 0.48660 to 0.48173, saving model to /content/drive/MyDrive/CNN_Models/tumor_classfication_1.h5\n",
            "72/72 [==============================] - 32s 440ms/step - loss: 0.1882 - accuracy: 0.9649 - val_loss: 0.4817 - val_accuracy: 0.8237 - lr: 1.0000e-06\n",
            "Epoch 35/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.1799 - accuracy: 0.9679\n",
            "Epoch 35: val_loss improved from 0.48173 to 0.47724, saving model to /content/drive/MyDrive/CNN_Models/tumor_classfication_1.h5\n",
            "72/72 [==============================] - 32s 446ms/step - loss: 0.1799 - accuracy: 0.9679 - val_loss: 0.4772 - val_accuracy: 0.8255 - lr: 1.0000e-06\n",
            "Epoch 36/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.1720 - accuracy: 0.9706\n",
            "Epoch 36: val_loss improved from 0.47724 to 0.47276, saving model to /content/drive/MyDrive/CNN_Models/tumor_classfication_1.h5\n",
            "72/72 [==============================] - 32s 440ms/step - loss: 0.1720 - accuracy: 0.9706 - val_loss: 0.4728 - val_accuracy: 0.8290 - lr: 1.0000e-06\n",
            "Epoch 37/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.1645 - accuracy: 0.9734\n",
            "Epoch 37: val_loss improved from 0.47276 to 0.46869, saving model to /content/drive/MyDrive/CNN_Models/tumor_classfication_1.h5\n",
            "72/72 [==============================] - 32s 444ms/step - loss: 0.1645 - accuracy: 0.9734 - val_loss: 0.4687 - val_accuracy: 0.8290 - lr: 1.0000e-06\n",
            "Epoch 38/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.1574 - accuracy: 0.9767\n",
            "Epoch 38: val_loss improved from 0.46869 to 0.46473, saving model to /content/drive/MyDrive/CNN_Models/tumor_classfication_1.h5\n",
            "72/72 [==============================] - 32s 439ms/step - loss: 0.1574 - accuracy: 0.9767 - val_loss: 0.4647 - val_accuracy: 0.8307 - lr: 1.0000e-06\n",
            "Epoch 39/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.1506 - accuracy: 0.9782\n",
            "Epoch 39: val_loss improved from 0.46473 to 0.46090, saving model to /content/drive/MyDrive/CNN_Models/tumor_classfication_1.h5\n",
            "72/72 [==============================] - 32s 444ms/step - loss: 0.1506 - accuracy: 0.9782 - val_loss: 0.4609 - val_accuracy: 0.8307 - lr: 1.0000e-06\n",
            "Epoch 40/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.1441 - accuracy: 0.9797\n",
            "Epoch 40: val_loss improved from 0.46090 to 0.45708, saving model to /content/drive/MyDrive/CNN_Models/tumor_classfication_1.h5\n",
            "72/72 [==============================] - 32s 447ms/step - loss: 0.1441 - accuracy: 0.9797 - val_loss: 0.4571 - val_accuracy: 0.8325 - lr: 1.0000e-06\n",
            "Epoch 41/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.1378 - accuracy: 0.9810\n",
            "Epoch 41: val_loss improved from 0.45708 to 0.45364, saving model to /content/drive/MyDrive/CNN_Models/tumor_classfication_1.h5\n",
            "72/72 [==============================] - 32s 445ms/step - loss: 0.1378 - accuracy: 0.9810 - val_loss: 0.4536 - val_accuracy: 0.8377 - lr: 1.0000e-06\n",
            "Epoch 42/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.1318 - accuracy: 0.9836\n",
            "Epoch 42: val_loss improved from 0.45364 to 0.45045, saving model to /content/drive/MyDrive/CNN_Models/tumor_classfication_1.h5\n",
            "72/72 [==============================] - 32s 445ms/step - loss: 0.1318 - accuracy: 0.9836 - val_loss: 0.4505 - val_accuracy: 0.8342 - lr: 1.0000e-06\n",
            "Epoch 43/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.1262 - accuracy: 0.9847\n",
            "Epoch 43: val_loss improved from 0.45045 to 0.44694, saving model to /content/drive/MyDrive/CNN_Models/tumor_classfication_1.h5\n",
            "72/72 [==============================] - 33s 454ms/step - loss: 0.1262 - accuracy: 0.9847 - val_loss: 0.4469 - val_accuracy: 0.8377 - lr: 1.0000e-06\n",
            "Epoch 44/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.1208 - accuracy: 0.9869\n",
            "Epoch 44: val_loss improved from 0.44694 to 0.44349, saving model to /content/drive/MyDrive/CNN_Models/tumor_classfication_1.h5\n",
            "72/72 [==============================] - 32s 444ms/step - loss: 0.1208 - accuracy: 0.9869 - val_loss: 0.4435 - val_accuracy: 0.8377 - lr: 1.0000e-06\n",
            "Epoch 45/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.1156 - accuracy: 0.9882\n",
            "Epoch 45: val_loss improved from 0.44349 to 0.44022, saving model to /content/drive/MyDrive/CNN_Models/tumor_classfication_1.h5\n",
            "72/72 [==============================] - 32s 446ms/step - loss: 0.1156 - accuracy: 0.9882 - val_loss: 0.4402 - val_accuracy: 0.8377 - lr: 1.0000e-06\n",
            "Epoch 46/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.1106 - accuracy: 0.9889\n",
            "Epoch 46: val_loss improved from 0.44022 to 0.43720, saving model to /content/drive/MyDrive/CNN_Models/tumor_classfication_1.h5\n",
            "72/72 [==============================] - 32s 441ms/step - loss: 0.1106 - accuracy: 0.9889 - val_loss: 0.4372 - val_accuracy: 0.8429 - lr: 1.0000e-06\n",
            "Epoch 47/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.1058 - accuracy: 0.9904\n",
            "Epoch 47: val_loss improved from 0.43720 to 0.43394, saving model to /content/drive/MyDrive/CNN_Models/tumor_classfication_1.h5\n",
            "72/72 [==============================] - 32s 442ms/step - loss: 0.1058 - accuracy: 0.9904 - val_loss: 0.4339 - val_accuracy: 0.8429 - lr: 1.0000e-06\n",
            "Epoch 48/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.1013 - accuracy: 0.9915\n",
            "Epoch 48: val_loss improved from 0.43394 to 0.43107, saving model to /content/drive/MyDrive/CNN_Models/tumor_classfication_1.h5\n",
            "72/72 [==============================] - 32s 447ms/step - loss: 0.1013 - accuracy: 0.9915 - val_loss: 0.4311 - val_accuracy: 0.8464 - lr: 1.0000e-06\n",
            "Epoch 49/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.0970 - accuracy: 0.9919\n",
            "Epoch 49: val_loss improved from 0.43107 to 0.42830, saving model to /content/drive/MyDrive/CNN_Models/tumor_classfication_1.h5\n",
            "72/72 [==============================] - 33s 457ms/step - loss: 0.0970 - accuracy: 0.9919 - val_loss: 0.4283 - val_accuracy: 0.8447 - lr: 1.0000e-06\n",
            "Epoch 50/50\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.0928 - accuracy: 0.9928\n",
            "Epoch 50: val_loss improved from 0.42830 to 0.42555, saving model to /content/drive/MyDrive/CNN_Models/tumor_classfication_1.h5\n",
            "72/72 [==============================] - 32s 440ms/step - loss: 0.0928 - accuracy: 0.9928 - val_loss: 0.4255 - val_accuracy: 0.8464 - lr: 1.0000e-06\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb5964ff640>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "model.fit(\n",
        "    train_df,\n",
        "    validation_data=valid_df,\n",
        "    epochs=50,\n",
        "    batch_size=64,\n",
        "    callbacks=callbacks\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Evaluation"
      ],
      "metadata": {
        "id": "rzC0dtXU4hfZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69Juq8B28Ddd",
        "outputId": "10d96d5a-ef46-4a92-e0f5-d237855c14b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9/9 [==============================] - 4s 393ms/step - loss: 0.4298 - accuracy: 0.8394\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.42976856231689453, 0.8394415378570557]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "model.evaluate(test_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model evaluation with confusion matrix"
      ],
      "metadata": {
        "id": "GfO1icWQ46jq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#plot confusion matrix\n",
        "def plt_confusion_matrix(cm, classes, normalize=False, title=\"Confusion Matrix\", cmap=plt.cm.Blues):\n",
        "  plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "  plt.title(title)\n",
        "  plt.colorbar()\n",
        "  tick_mark = np.arange(len(classes))\n",
        "  plt.xticks(tick_mark, classes, rotation=45)\n",
        "  plt.yticks(tick_mark, classes)\n",
        "\n",
        "  if normalize:\n",
        "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.axis]\n",
        "    print(\"normalized confusion matrix\")\n",
        "\n",
        "  else:\n",
        "    print(\"confusion matrix without normalization\")\n",
        "  \n",
        "  thresh = cm.max() / 2\n",
        "  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "    plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.xlabel(\"predicted label\")\n",
        "  plt.ylabel(\"True label\")"
      ],
      "metadata": {
        "id": "xi8Y4Vma4ndZ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prediction\n",
        "prediction = model.predict(test_df, verbose=0)"
      ],
      "metadata": {
        "id": "ZvSFS2ps4_aB"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#scale the predicted value\n",
        "np.around(prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqrvHyUW5DXp",
        "outputId": "a4990a2e-cd97-4787-bc83-3ea3b5fb34b2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 1.],\n",
              "       [0., 1., 0., 0.],\n",
              "       [0., 0., 0., 1.],\n",
              "       ...,\n",
              "       [0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0.],\n",
              "       [0., 0., 0., 1.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#get the max value \n",
        "y_pred_classes = np.argmax(prediction, axis=1)"
      ],
      "metadata": {
        "id": "Mf0O48My5GAR"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function for get labels of test set\n",
        "def get_test_data_class(test_path):\n",
        "  names = []\n",
        "  for i in test_path:\n",
        "    name = i.split(\"/\")[-2]\n",
        "    name_idx = class_names.index(name)\n",
        "    names.append(name_idx)\n",
        "  names = np.array(names, dtype=np.int32)\n",
        "  return names"
      ],
      "metadata": {
        "id": "sTzqba7S5ISp"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = get_test_data_class(x_test)"
      ],
      "metadata": {
        "id": "1RxhMN-Z5Kmq"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#confusion matrix\n",
        "cm = confusion_matrix(y_true=classes, y_pred=y_pred_classes)"
      ],
      "metadata": {
        "id": "QPpbthPn5MXB"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt_confusion_matrix(cm=cm, classes=class_names, title=\"confusion matrix\", )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "yoqiqoTV5QYp",
        "outputId": "6da26adf-5621-4a44-8bdd-9ad317923746"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "confusion matrix without normalization\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#save model architecture and weights separetely\n",
        "model_json = model.to_json()\n",
        "with open(\"/content/drive/MyDrive/CNN_Models/model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "model.save_weights(\"/content/drive/MyDrive/CNN_Models/tumor_model_83.h5\")"
      ],
      "metadata": {
        "id": "I-a9-ABe5SLB"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EpILXoUs6DY8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "mount_file_id": "19Ds0JypUoNjrzDde_24N0mkCfrv6g3ff",
      "authorship_tag": "ABX9TyNAC2eBJFVPNaBesyZLtYdM",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}